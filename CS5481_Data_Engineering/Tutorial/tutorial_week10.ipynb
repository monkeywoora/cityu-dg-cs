{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5481 Data Engineering: Tutorial on Representation Learning and Multimodal Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome everyone to Tutorial 10. Today we're focusing on two key concepts in deep learning: representation learning and multimodal learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match ResNet50 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Convert grayscale to RGB\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model using ResNet50 for feature extraction and a fully connected layer for classification\n",
    "class ResNet50Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet50Classifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50 and remove the classification layer\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        # Freeze the parameters of ResNet50\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Remove the final fully connected layer of ResNet50\n",
    "        self.features = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Add a fully connected layer for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using ResNet50\n",
    "        features = self.features(x)\n",
    "        # Classify using the fully connected layer\n",
    "        output = self.fc(features)\n",
    "        return output\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet50Classifier(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': running_loss / (progress_bar.n + 1),\n",
    "                'acc': 100 * correct / total\n",
    "            })\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100*correct/total:.2f}%')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "print(f\"Training on {device}\")\n",
    "train(model, train_loader, criterion, optimizer, device, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader, device)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'resnet50_mnist_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multimodal Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d339a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Configuration parameters\n",
    "vocab_threshold = 4\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "\n",
    "# Simple vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n",
    "        self.idx2word = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n",
    "        self.idx = 4\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Simple tokenization function - using space splitting and punctuation handling\n",
    "def simple_tokenize(text):\n",
    "    # Treat punctuation as separate tokens\n",
    "    for punct in ',.!?;:':\n",
    "        text = text.replace(punct, f' {punct} ')\n",
    "    # Split by spaces\n",
    "    return [token.lower() for token in text.split() if token]\n",
    "\n",
    "# Build simple vocabulary\n",
    "def build_vocab(token_file):\n",
    "    vocab = Vocabulary()\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Read caption file\n",
    "    with open(token_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                caption = parts[1].lower()\n",
    "                tokens = simple_tokenize(caption)\n",
    "                for token in tokens:\n",
    "                    if token not in word_counts:\n",
    "                        word_counts[token] = 0\n",
    "                    word_counts[token] += 1\n",
    "    \n",
    "    # Add words that appear more than threshold times\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= vocab_threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "# Flickr8k dataset class\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, img_dir, caption_file, img_list_file, vocab, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Read image list\n",
    "        with open(img_list_file, 'r') as f:\n",
    "            self.img_list = [line.strip() for line in f]\n",
    "        \n",
    "        # Read and map captions\n",
    "        self.captions = {}\n",
    "        with open(caption_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    img_id = parts[0].split('#')[0]\n",
    "                    caption = parts[1].lower()\n",
    "                    if img_id in self.img_list:\n",
    "                        if img_id not in self.captions:\n",
    "                            self.captions[img_id] = []\n",
    "                        self.captions[img_id].append(caption)\n",
    "        \n",
    "        # Keep only images with captions\n",
    "        self.img_list = [img for img in self.img_list if img in self.captions]\n",
    "        print(f\"Dataset contains {len(self.img_list)} images\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_list[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Randomly select one caption\n",
    "        caption = np.random.choice(self.captions[img_id])\n",
    "        \n",
    "        # Tokenize and convert to indices\n",
    "        tokens = simple_tokenize(caption)\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<start>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('<end>'))\n",
    "        \n",
    "        return image, torch.Tensor(caption).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "# Simplified collate function for DataLoader\n",
    "def collate_fn(data):\n",
    "    images, captions = zip(*data)\n",
    "    \n",
    "    # Get lengths and create padded tensor\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    \n",
    "    # Padding\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    return images, targets, lengths\n",
    "\n",
    "# Image encoder (ResNet50 simplified)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # Use pretrained ResNet50\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the final fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# Text decoder (simplified, avoiding pack_padded_sequence)\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        # Embed all caption words\n",
    "        embeddings = self.embed(captions)\n",
    "        \n",
    "        # Add image features as first input\n",
    "        batch_size = features.size(0)\n",
    "        features = features.unsqueeze(1)\n",
    "        embeddings = torch.cat((features, embeddings[:, :-1]), 1)\n",
    "        \n",
    "        # LSTM forward pass (without pack_padded_sequence)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        \n",
    "        # Predict next word\n",
    "        outputs = self.linear(hiddens)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, max_len=20):\n",
    "        \"\"\"Generate captions (inference mode)\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        states = None\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            \n",
    "            # Stop condition\n",
    "            if (predicted == 2).sum() == predicted.size(0): # All samples reached <end>\n",
    "                break\n",
    "                \n",
    "            # Input for next time step\n",
    "            inputs = self.embed(predicted).unsqueeze(1)\n",
    "        \n",
    "        return torch.stack(sampled_ids, 1)\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define paths\n",
    "    data_dir = 'flickr8k_data'\n",
    "    img_dir = os.path.join(data_dir, 'images')\n",
    "    token_file = os.path.join(data_dir, 'Flickr8k.token.txt')\n",
    "    train_file = os.path.join(data_dir, 'Flickr_8k.trainImages.txt')\n",
    "    val_file = os.path.join(data_dir, 'Flickr_8k.devImages.txt')\n",
    "    \n",
    "    # Image transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = build_vocab(token_file)\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = Flickr8kDataset(img_dir, token_file, train_file, vocab, transform)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_dataset = Flickr8kDataset(img_dir, token_file, val_file, vocab, transform)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    encoder = EncoderCNN(embedding_dim).to(device)\n",
    "    decoder = DecoderRNN(embedding_dim, hidden_dim, len(vocab)).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<pad>'])\n",
    "    params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            \n",
    "            # Calculate loss - using reshape to match dimensions\n",
    "            outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "            targets = captions.reshape(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Show progress\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Generate sample captions\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of validation data\n",
    "            images, _, _ = next(iter(val_loader))\n",
    "            images = images[:3].to(device)  # Use only 3 images as examples\n",
    "            \n",
    "            # Generate captions\n",
    "            features = encoder(images)\n",
    "            sampled_ids = decoder.sample(features)\n",
    "            \n",
    "            # Convert indices to words\n",
    "            for i in range(len(sampled_ids)):\n",
    "                sampled_caption = []\n",
    "                for word_id in sampled_ids[i].cpu().numpy():\n",
    "                    word = vocab.idx2word.get(word_id, '<unk>')\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                    if word not in ['<start>', '<pad>']:\n",
    "                        sampled_caption.append(word)\n",
    "                \n",
    "                print(f\"Generated caption {i+1}: {' '.join(sampled_caption)}\")\n",
    "    \n",
    "    # Save models\n",
    "    torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "    torch.save(decoder.state_dict(), 'decoder.pth')\n",
    "    print(\"Models saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FracNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
