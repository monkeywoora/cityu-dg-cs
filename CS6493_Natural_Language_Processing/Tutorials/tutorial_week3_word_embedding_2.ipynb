{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD0bad8EnUEC"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Word Embedding is an important term in Natural Language Processing and a significant breakthrough in deep learning that solved many problems. In this article, we’ll be looking into what pre-trained word embeddings in NLP are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEYnaplxnUEE"
      },
      "source": [
        "Table of Content\n",
        "- Word Embeddings\n",
        "- Challenges in building word embedding from scratch\n",
        "- Pre Trained Word Embeddings\n",
        "- Word2Vec\n",
        "- GloVe\n",
        "- BERT Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMNRdRD0nUEE"
      },
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Word embedding is an approach in Natural language Processing where raw text gets converted to numbers/vectors. As deep learning models only take numerical input this technique becomes important to process the raw data. It helps in capturing the semantic meaning as well as the context of the words. A real-valued vector with various dimensions represents each word.\n",
        "\n",
        "There are certain methods of generating word embeddings such as BOW (Bag of words), TF-IDF, Glove, BERT embeddings, etc. The earlier methods only converted the words without extracting the semantic relationship and context. But the recent ones such as BERT embeddings, which is a pre-trained word embedding model capture the full context of the word as well as the semantic relationships of the word within the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-_T8SNVnUEF"
      },
      "source": [
        "## Challenges in building word embedding from scratch\n",
        "\n",
        "Training word embeddings from scratch is possible but it is quite challenging due to large trainable parameters and sparsity of training data. These models need to be trained on a large number of datasets with rich vocabulary and as there are large number of parameters, it makes the training slower. So, it’s quite challenging to train a word embedding model on an individual level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G38FbrpnUEF"
      },
      "source": [
        "## Pre Trained Word Embeddings\n",
        "\n",
        "There’s a solution to the above problem, i.e., using pre-trained word embeddings. Pre-trained word embeddings are trained on large datasets and capture the syntactic as well as semantic meaning of the words. This technique is known as transfer learning in which you take a model which is trained on large datasets and use that model on your own similar tasks.\n",
        "\n",
        "There are two broad classifications of pre trained word embeddings – word-level and character-level. We’ll be looking into two types of word-level embeddings i.e. Word2Vec and GloVe and how they can be used to generate embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p3aGZ4WnUEG"
      },
      "source": [
        "## Word2Vec\n",
        "\n",
        "Word2Vec is one of the most popular pre trained word embeddings developed by Google. It is trained on Good news dataset which is an extensive dataset. As the name suggests, it represents each word with a collection of integers known as a vector. The vectors are calculated such that they show the semantic relation between words.\n",
        "\n",
        "A popular example of how semantic relation is made is the king queen example:\n",
        "\n",
        "```\n",
        "King - Man + Woman ~ Queen\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# https://jalammar.github.io/illustrated-word2vec/  The best tutorial on word2vec  \n",
        "# https://projector.tensorflow.org/"
      ],
      "metadata": {
        "id": "WQPGnwDLhxn5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAa0oVCQnUEG"
      },
      "source": [
        "Word2vec is a feed-forward neural network which consists of two main models – Continuous Bag-of-Words (CBOW) and Skip-gram model. The continuous bag of words model learns the target word from the adjacent words whereas in the skip-gram model, the model learns the adjacent words from the target word. They are completely opposite of each other.\n",
        "\n",
        "Firstly, the size of context window is defined. Context window is a sliding window which runs through the whole text one word at a time. It basically refers to the number of words appearing on the right and left side of the focus word. eg. if size of the context window is set to 2, then it will include 2 words on the right as well as left of the focus word.\n",
        "\n",
        "Focus word is our target word for which we want to create the embedding / vector representation. Generally, focus word is the middle word but in the example below we’re taking last word as our target word. The neighbouring words are the words that appear in the context window. These words help in capturing the context of the whole sentence. Let’s understand this with the help of an example.\n",
        "\n",
        "Suppose we have a sentence – “He poured himself a cup of coffee”. The target word here is “himself”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOhJtwtFnUEG"
      },
      "source": [
        "### Continuous Bag-Of-Words\n",
        "\n",
        "input = [“He”, “poured”, “a”, “cup”]\n",
        "\n",
        "output = [“himself”]\n",
        "\n",
        "### Skip-gram model\n",
        "\n",
        "input = [“himself”]\n",
        "\n",
        "output = [“He”, “poured”, “a”, “cup”]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwQ85OMqnUEH"
      },
      "source": [
        "To generate word embeddings using pre trained word word2vec embeddings, first download the model bin file from here. Then import all the necessary libraries needed such as gensim (will be used for initialising the pre trained model from the bin file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGLhp6nJnUEH",
        "outputId": "98db5a95-f51c-44d9-c029-ffe4b900f675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHG8E-x4nUEH",
        "outputId": "de4a33d9-8fd6-46b7-dcc7-69c492f0415e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03583806\n",
            "0.22942673\n"
          ]
        }
      ],
      "source": [
        "#import gensim library\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "#replace with the path where you have downloaded your model.\n",
        "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
        "pretrained_model_path = '/content/drive/MyDrive/tutorial_data/GoogleNews-vectors-negative300.bin.gz'\n",
        "#initialise the pre trained model using load_word2vec_format from gensim module.\n",
        "word_vectors = KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)\n",
        "\n",
        "# Calculate cosine similarity between word pairs\n",
        "word1 = \"early\"\n",
        "word2 = \"seats\"\n",
        "#calculate the similarity\n",
        "similarity1 = word_vectors.similarity(word1, word2)\n",
        "#print final value\n",
        "print(similarity1)\n",
        "\n",
        "word3 = \"king\"\n",
        "word4 = \"man\"\n",
        "#calculate the similarity\n",
        "similarity2 = word_vectors.similarity(word3, word4)\n",
        "#print final value\n",
        "print(similarity2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJY17xuynUEI"
      },
      "source": [
        "The above code initialises word2vec model using gensim library. It calculates the cosine similarity between words. As you can see the second value is comparatively larger than the first one (these values ranges from -1 to 1), so this means that the words “king” and “man” have more similarity.\n",
        "\n",
        "We can also find words which are most similar to the given word as parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWHpp22SnUEJ",
        "outputId": "ea33fbb8-5b86-4796-aa0c-9053dde3146c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most similar words to \"King\" are : [('Jackson', 0.5326348543167114), ('Prince', 0.5306329727172852), ('Tupou_V.', 0.5292826294898987), ('KIng', 0.5227501392364502), ('e_mail_robert.king_@', 0.5173623561859131), ('king', 0.5158917903900146), ('Queen', 0.5157250165939331), ('Geoffrey_Rush_Exit', 0.49920955300331116), ('prosecutor_Dan_Satterberg', 0.49850785732269287), ('NECN_Alison', 0.49128594994544983)]\n"
          ]
        }
      ],
      "source": [
        "# finding most similar word embeddings with King\n",
        "king = word_vectors.most_similar('King')\n",
        "print(f'Top 10 most similar words to \"King\" are : {king}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk0pc2ZanUEJ"
      },
      "source": [
        "## GloVe\n",
        "\n",
        "Given by Stanford, GloVe stands for Global Vectors for Word Representation. It is a popular word embedding model which works on the basic idea of deriving the relationship between words using statistics. It is a count based model that employs co-occurrence matrix. A co-occurrence matrix tells how often two words are occurring globally. Each value is a count of a pair of words occurring together.\n",
        "\n",
        "Glove basically deals with the spaces where the distance between words is linked to to their semantic similarity. It has properties of the global matrix factorisation and the local context window technique. Training of the model is based on the global word-word co-occurrence data from a corpus, and the resultant representations results into linear substructure of the vector space\n",
        "\n",
        "GloVe calculates the co-occurrence probabilities for each word pair. It divides the co-occurrence counts by the total number of co-occurrences for each word:\n",
        "\n",
        "$$F(w_{i}, w_{j}, w_{k}) = \\frac {P_{ik}}{P_{jk}}$$      \n",
        "\n",
        "For example, the co-occurrence probability of “cat” and “mouse” is calculated as: Co-occurrence Probability(“cat”, “mouse”) = Count(“cat” and “mouse”) / Total Co-occurrences(“cat”)\n",
        "\n",
        "In this case:\n",
        "\n",
        "Count(\"cat\" and \"mouse\") = 1\n",
        "\n",
        "Total Co-occurrences(\"cat\") = 2 (with \"chases\" and \"mouse\")\n",
        "\n",
        "So, Co-occurrence Probability(\"cat\", \"mouse\") = 1 / 2 = 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1y_sEVnUEK"
      },
      "source": [
        "GloVe Model Building\n",
        "\n",
        "Firstly, download gloVe 6B embeddings from this site. Then unzip the file and add the file to the same folder as your code. There are many variations of the 6B model but we’ll using the glove.6B.50d."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "_6jViM-mt9rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "def find_top_n_nearest_words(target_word, embeddings, n=5):\n",
        "    \"\"\"\n",
        "    Find the top N nearest words to the target word based on cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        target_word (str): The target word.\n",
        "        embeddings (dict): A dictionary of word embeddings (word -> vector).\n",
        "        n (int): Number of nearest words to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples (word, similarity) for the top N nearest words.\n",
        "    \"\"\"\n",
        "    if target_word not in embeddings:\n",
        "        print(f\"'{target_word}' not found in the vocabulary.\")\n",
        "        return None\n",
        "\n",
        "    target_vector = embeddings[target_word]\n",
        "    similarities = []\n",
        "\n",
        "    for word, vector in embeddings.items():\n",
        "        if word == target_word:\n",
        "            continue  # Skip the target word itself\n",
        "        similarity = 1 - cosine(target_vector, vector)  # Cosine similarity\n",
        "        similarities.append((word, similarity))\n",
        "\n",
        "    # Sort by similarity in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the top N nearest words\n",
        "    return similarities[:n]\n",
        "\n",
        "# Calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "# Normalize vectors (optional but recommended)\n",
        "def normalize_vectors(embeddings):\n",
        "    for word in embeddings:\n",
        "        embeddings[word] /= np.linalg.norm(embeddings[word])\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/tutorial_data/glove.6B.50d.txt'\n",
        "embeddings = load_glove_embeddings(file_path)\n",
        "embeddings = normalize_vectors(embeddings)"
      ],
      "metadata": {
        "id": "ktt8txYQm1kD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_word = \"king\"\n",
        "top_n_words = find_top_n_nearest_words(target_word, embeddings, n=10)\n",
        "\n",
        "if top_n_words:\n",
        "    print(f\"Top {len(top_n_words)} nearest words to '{target_word}':\")\n",
        "    for word, similarity in top_n_words:\n",
        "        print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "befCg90Ko5Sf",
        "outputId": "f98fa8dd-f9a3-4ef0-e7e5-df5ae0813109"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 nearest words to 'king':\n",
            "prince: 0.8236\n",
            "queen: 0.7839\n",
            "ii: 0.7746\n",
            "emperor: 0.7736\n",
            "son: 0.7667\n",
            "uncle: 0.7627\n",
            "kingdom: 0.7542\n",
            "throne: 0.7540\n",
            "brother: 0.7492\n",
            "ruler: 0.7434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# exercise   \n",
        "calculate cosine similarity of previous examples"
      ],
      "metadata": {
        "id": "7hKTQTIGntjE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oJNy23Rxl6VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOIjdwuTmZK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvV_CY_gnUEK"
      },
      "source": [
        "## BERT Embeddings\n",
        "\n",
        "Another important pre trained transformer based model is by Google known as BERT or Bidirectional Encoder Representations from Transformers. It can be used to extract high quality language features from raw text or can be fine-tuned on own data to perform specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u7HTM_3nUEL"
      },
      "source": [
        "BERT’s architecture consists of only encoders and input received is a sequence of tokens i.e. Token embeddings, Segment embeddings and Positional embeddings. The main idea is to mask a few words in a sentence and task the model to predict the masked words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbFpum_NnUEL"
      },
      "source": [
        "BERT\n",
        "\n",
        "Firstly, install the transformers library as we’ll be using pytorch and transformers for implementing this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXEal51OnUEL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hra1c91NnUEL",
        "outputId": "7268e219-2270-4ab4-8e87-9375680e1057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'king' and 'queen': 0.7545\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "\n",
        "# Function to get BERT embeddings for a word in a sentence\n",
        "def get_bert_embeddings(text, target_word):\n",
        "    # Tokenize input text\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensor = torch.tensor([segments_ids])\n",
        "\n",
        "    # Get embeddings from BERT\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensor)\n",
        "        hidden_states = outputs.hidden_states  # Tuple of all hidden states\n",
        "\n",
        "    # Extract embeddings for the target word\n",
        "    # Use the last 4 layers (common practice for BERT embeddings)\n",
        "    token_embeddings = torch.stack(hidden_states[-4:], dim=0)\n",
        "    token_embeddings = torch.mean(token_embeddings, dim=0)  # Average over layers\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=0)  # Remove batch dimension\n",
        "\n",
        "    # Find the index of the target word\n",
        "    try:\n",
        "        target_index = tokenized_text.index(target_word)\n",
        "    except ValueError:\n",
        "        print(f\"'{target_word}' not found in the tokenized text.\")\n",
        "        return None\n",
        "\n",
        "    # Get the embedding for the target word\n",
        "    target_embedding = token_embeddings[target_index].numpy()\n",
        "    return target_embedding\n",
        "\n",
        "# Function to compute cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return 1 - cosine(vec1, vec2)\n",
        "\n",
        "# Example usage\n",
        "text = \"The king and queen ruled the kingdom.\"\n",
        "word1 = \"king\"\n",
        "word2 = \"queen\"\n",
        "\n",
        "# Get embeddings for \"king\" and \"queen\"\n",
        "embedding_king = get_bert_embeddings(text, word1)\n",
        "embedding_queen = get_bert_embeddings(text, word2)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity(embedding_king, embedding_queen)\n",
        "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings for two words in the same sentence\n",
        "embedding_king = get_bert_embeddings(\"The king ruled the kingdom.\", \"king\")\n",
        "embedding_queen = get_bert_embeddings(\"The queen ruled the kingdom.\", \"queen\")\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = 1 - cosine(embedding_king, embedding_queen)\n",
        "print(f\"Cosine similarity between 'king' and 'queen': {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xyxuqh8p5vf",
        "outputId": "68a08e20-821e-4c53-dde0-3088b8d693e9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'king' and 'queen': 0.8322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text1 = \"The king ruled the kingdom.\"\n",
        "text2 = \"I ate an apple for breakfast.\"\n",
        "\n",
        "# Get embeddings for \"king\" and \"apple\"\n",
        "embedding_king = get_bert_embeddings(text1, \"king\")\n",
        "embedding_apple = get_bert_embeddings(text2, \"apple\")\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity(embedding_king, embedding_apple)\n",
        "print(f\"Cosine similarity between 'king' and 'apple': {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0ZU31lKrF2L",
        "outputId": "9482b93d-09cb-48f1-a778-9ed7b9e90c1f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'king' and 'apple': 0.3491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJGulD-SqATG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.0.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}